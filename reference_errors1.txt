REFERENCE VERIFICATION ERRORS

PAPER: Larimar: Large Language Models with Episodic Memory Control
ArXiv ID: 2403.11901v4
URL: https://arxiv.org/abs/2403.11901v4
Authors: Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, Pin-Yu Chen
Year: 2024
--------------------------------------------------------------------------------
REFERENCE: Language models are few-shot learners
Type: ❌ author
Details: First author mismatch: 'Tom Brown' vs 'T. B. Brown'

RAW REFERENCE TEXT:
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.#Language models are few-shot learners#Advances in neural information processing systems#2020#

VERIFIED URL:
  https://arxiv.org/abs/2005.14165

CORRECTED REFERENCE:
T. B. Brown, Benjamin F. Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey C.S. Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric J. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack A. Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. "Language Models are Few-Shot Learners".

================================================================================
REFERENCE: Language models as knowledge bases?
Type: ⚠️ venue
Details: Venue mismatch: cited as 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)' but actually 'Conference on Empirical Methods in Natural Language Processing'

RAW REFERENCE TEXT:
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller#Language models as knowledge bases?#Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)#2019#

VERIFIED URL:
  https://arxiv.org/abs/1909.01066

CORRECTED REFERENCE:
F. Petroni, Tim Rocktäschel, Patrick Lewis, A. Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel. (2019). "Language Models as Knowledge Bases?". In Conference on Empirical Methods in Natural Language Processing. https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3.

================================================================================
REFERENCE: Overcoming catastrophic forgetting in neural networks
Type: ❌ multiple
Details: - Year mismatch: cited as 2017 but actually 2016
- Venue mismatch: cited as 'Proceedings of the national academy of sciences' but actually 'Proceedings of the National Academy of Sciences of the United States of America'

RAW REFERENCE TEXT:
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.#Overcoming catastrophic forgetting in neural networks#Proceedings of the national academy of sciences#2017#

VERIFIED URL:
  https://arxiv.org/abs/1612.00796

CORRECTED REFERENCE:
J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R. Hadsell. (2016). "Overcoming catastrophic forgetting in neural networks". In Proceedings of the National Academy of Sciences of the United States of America. https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9.

================================================================================
REFERENCE: Large language models with controllable working memory
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2211.05110

RAW REFERENCE TEXT:
Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, Sanjiv Kumar#Large language models with controllable working memory#2022#

VERIFIED URL:
  https://arxiv.org/abs/2211.05110

CORRECTED REFERENCE:
Daliang Li, A. Rawat, M. Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, Surinder Kumar. (2022). "Large Language Models with Controllable Working Memory". In Annual Meeting of the Association for Computational Linguistics. https://www.semanticscholar.org/paper/ee8de585183763ff64cb3c81ecda2fc75fa81507.

================================================================================
REFERENCE: Lost in the middle: How language models use long contexts
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2307.03172

RAW REFERENCE TEXT:
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang#Lost in the middle: How language models use long contexts#2023#

VERIFIED URL:
  https://arxiv.org/abs/2307.03172

CORRECTED REFERENCE:
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, F. Petroni, Percy Liang. (2023). "Lost in the Middle: How Language Models Use Long Contexts". In Transactions of the Association for Computational Linguistics. https://www.semanticscholar.org/paper/1733eb7792f7a43dd21f51f4d1017a1bffd217b5.

================================================================================
REFERENCE: Can sensitive information be deleted from llms? objectives for defending against extraction attacks
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2309.17410

RAW REFERENCE TEXT:
Vaidehi Patil, Peter Hase, Mohit Bansal#Can sensitive information be deleted from llms? objectives for defending against extraction attacks#2023#

VERIFIED URL:
  https://arxiv.org/abs/2309.17410

CORRECTED REFERENCE:
Vaidehi Patil, Peter Hase, Mohit Bansal. (2023). "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/46eea7d651420e60f9b1393e3f5eda14cbff7a2a.

================================================================================
REFERENCE: Regional synapse gain and loss accompany memory formation in larval zebrafish
Type: ⚠️ venue
Details: Venue mismatch: cited as 'Proceedings of the National Academy of Sciences' but actually 'Proceedings of the National Academy of Sciences of the United States of America'

RAW REFERENCE TEXT:
William P Dempsey, Zhuowei Du, Anna Nadtochiy, Colton D Smith, Karl Czajkowski, Andrey Andreev, Drew N Robson, Jennifer M Li, Serina Applebaum, Thai V Truong, et al.#Regional synapse gain and loss accompany memory formation in larval zebrafish#Proceedings of the National Academy of Sciences#2022#

VERIFIED URL:
  https://doi.org/10.1073/pnas.2107661119

CORRECTED REFERENCE:
William P. Dempsey, Zhuowei Du, Anna Nadtochiy, Colton D Smith, K. Czajkowski, Andrey Andreev, D. Robson, Jennifer M. Li, Serina S Applebaum, T. Truong, C. Kesselman, S. Fraser, Don B. Arnold. (2022). "Regional synapse gain and loss accompany memory formation in larval zebrafish". In Proceedings of the National Academy of Sciences of the United States of America. https://www.semanticscholar.org/paper/fad430a1134a3a32e6a6d92bc67d83caa80fcc2e.

================================================================================
REFERENCE: Model-free episodic control
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1606.04460

RAW REFERENCE TEXT:
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, Demis Hassabis#Model-free episodic control#2016#

VERIFIED URL:
  https://arxiv.org/abs/1606.04460

CORRECTED REFERENCE:
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack W. Rae, Daan Wierstra, Demis Hassabis. (2016). "Model-Free Episodic Control". In arXiv.org. https://www.semanticscholar.org/paper/ba378579fb44007db9f02699889721dcd2b5b3a0.

================================================================================
REFERENCE: Organizing memories for generalization in complementary learning systems
Type: ❌ multiple
Details: - Year mismatch: cited as 2023 but actually 2021
- Venue mismatch: cited as 'Nature neuroscience' but actually 'bioRxiv'

RAW REFERENCE TEXT:
Weinan Sun, Madhu Advani, Nelson Spruston, Andrew Saxe, James E Fitzgerald#Organizing memories for generalization in complementary learning systems#Nature neuroscience#2023#

VERIFIED URL:
  https://www.nature.com/articles/s41593-023-01382-9.pdf

CORRECTED REFERENCE:
Weinan Sun, Madhu S. Advani, N. Spruston, Andrew M. Saxe, James E. Fitzgerald. (2021). "Organizing memories for generalization in complementary learning systems". In bioRxiv. https://www.semanticscholar.org/paper/a105819206fc7db56c71aaaa07c0b12d959c5816.

================================================================================
REFERENCE: The kanerva machine: A generative distributed memory
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1804.01756

RAW REFERENCE TEXT:
Yan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap#The kanerva machine: A generative distributed memory#2018a#

VERIFIED URL:
  https://arxiv.org/abs/1804.01756

CORRECTED REFERENCE:
Yan Wu, Greg Wayne, Alex Graves, T. Lillicrap. (2018). "The Kanerva Machine: A Generative Distributed Memory". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/5b2c83f41eacdf95e8b38300d2926ac37ea4709e.

================================================================================
REFERENCE: Generative pseudo-inverse memory
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
Kha Pham, Hung Le, Man Ngo, Truyen Tran, Bao Ho, Svetha Venkatesh#Generative pseudo-inverse memory#International Conference on Learning Representations#2021#

================================================================================
REFERENCE: Mass-editing memory in a transformer
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, David Bau#Mass-editing memory in a transformer#The Eleventh International Conference on Learning Representations#2023#

================================================================================
REFERENCE: Pointer sentinel mixture models
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1609.07843

RAW REFERENCE TEXT:
Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher#Pointer sentinel mixture models#2016#

VERIFIED URL:
  https://arxiv.org/abs/1609.07843

CORRECTED REFERENCE:
Stephen Merity, Caiming Xiong, James Bradbury, R. Socher. (2016). "Pointer Sentinel Mixture Models". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd.

================================================================================
REFERENCE: Aging with grace: Lifelong model editing with discrete key-value adaptors
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2022' but actually 'Neural Information Processing Systems'

RAW REFERENCE TEXT:
Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi#Aging with grace: Lifelong model editing with discrete key-value adaptors#arXiv preprint#2022#arXiv:2211.11031

VERIFIED URL:
  https://arxiv.org/abs/2211.11031

CORRECTED REFERENCE:
Thomas Hartvigsen, S. Sankaranarayanan, Hamid Palangi, Yoon Kim, M. Ghassemi. (2022). "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors". In Neural Information Processing Systems. https://www.semanticscholar.org/paper/560b1bc012588731b26748e33236570df777baa0.

================================================================================
REFERENCE: Can we edit factual knowledge by in-context learning?
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2305.12740

RAW REFERENCE TEXT:
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang#Can we edit factual knowledge by in-context learning?#2023#

VERIFIED URL:
  https://arxiv.org/abs/2305.12740

CORRECTED REFERENCE:
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang. (2023). "Can We Edit Factual Knowledge by In-Context Learning?". In Conference on Empirical Methods in Natural Language Processing. https://www.semanticscholar.org/paper/ff2a0fb125e7f03428420230c6ecbeafd4cf07a8.

================================================================================
REFERENCE: Zero-shot relation extraction via reading comprehension
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2017' but actually 'Conference on Computational Natural Language Learning'

RAW REFERENCE TEXT:
Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer#Zero-shot relation extraction via reading comprehension#arXiv preprint#2017#arXiv:1706.04115

VERIFIED URL:
  https://arxiv.org/abs/1706.04115

CORRECTED REFERENCE:
Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer. (2017). "Zero-Shot Relation Extraction via Reading Comprehension". In Conference on Computational Natural Language Learning. https://www.semanticscholar.org/paper/fa025e5d117929361bcf798437957762eb5bb6d4.

================================================================================
REFERENCE: 2023 in review fast facts
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
CNN#2023 in review fast facts#2023#https://www.cnn.com/2023/11/13/us/2023-in-review-fast-facts/index.html

================================================================================
REFERENCE: Supersizing transformers: Going beyond rag with extended minds for llms
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
Phoebe Klett, Thomas Ahle#Supersizing transformers: Going beyond rag with extended minds for llms#The Normal Blog#2023#https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html

================================================================================
REFERENCE: In-context learning and induction heads
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2209.11895

RAW REFERENCE TEXT:
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah#In-context learning and induction heads#2022#

VERIFIED URL:
  https://arxiv.org/abs/2209.11895

CORRECTED REFERENCE:
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, T. Henighan, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, John Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom B. Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah. (2022). "In-context Learning and Induction Heads". In arXiv.org. https://www.semanticscholar.org/paper/c90a99eeb57019732a6cc996bb9eaf13faedf00f.

================================================================================
REFERENCE: Efficiently modeling long sequences with structured state spaces
Type: ❌ multiple
Details: - Year mismatch: cited as 2022 but actually 2021
- Reference should include arXiv URL: https://arxiv.org/abs/2111.00396

RAW REFERENCE TEXT:
Albert Gu, Karan Goel, Christopher R é#Efficiently modeling long sequences with structured state spaces#2022#

VERIFIED URL:
  https://arxiv.org/abs/2111.00396

CORRECTED REFERENCE:
Albert Gu, Karan Goel, Christopher R'e. (2021). "Efficiently Modeling Long Sequences with Structured State Spaces". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51.

================================================================================
REFERENCE: Repeat after me: Transformers are better than state space models at copying
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2402.01032

RAW REFERENCE TEXT:
Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach#Repeat after me: Transformers are better than state space models at copying#2024#

VERIFIED URL:
  https://arxiv.org/abs/2402.01032

CORRECTED REFERENCE:
Samy Jelassi, David Brandfonbrener, S. Kakade, Eran Malach. (2024). "Repeat After Me: Transformers are Better than State Space Models at Copying". In International Conference on Machine Learning. https://www.semanticscholar.org/paper/189fde3f4dfa105bb51472a8945618f395919560.

================================================================================
REFERENCE: Key-value memory networks for directly reading documents
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1606.03126

RAW REFERENCE TEXT:
Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, Jason Weston#Key-value memory networks for directly reading documents#2016#

VERIFIED URL:
  https://arxiv.org/abs/1606.03126

CORRECTED REFERENCE:
Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, J. Weston. (2016). "Key-Value Memory Networks for Directly Reading Documents". In Conference on Empirical Methods in Natural Language Processing. https://www.semanticscholar.org/paper/bba5f2852b1db8a18004eb7328efa5e1d57cc62a.

================================================================================
REFERENCE: Learning attractor dynamics for generative memory
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1811.09556

RAW REFERENCE TEXT:
Yan Wu, Greg Wayne, Karol Gregor, Timothy Lillicrap#Learning attractor dynamics for generative memory#2018b#

VERIFIED URL:
  https://arxiv.org/abs/1811.09556

CORRECTED REFERENCE:
Yan Wu, Greg Wayne, Karol Gregor, T. Lillicrap. (2018). "Learning Attractor Dynamics for Generative Memory". In Neural Information Processing Systems. https://www.semanticscholar.org/paper/c587b8a94352831d8f9d7e567eac7eb513e8d589.

================================================================================
REFERENCE: Kanerva++: extending the kanerva machine with differentiable, locally block allocated latent memory
Type: ❌ multiple
Details: - Year mismatch: cited as 2022 but actually 2021
- Reference should include arXiv URL: https://arxiv.org/abs/2103.03905

RAW REFERENCE TEXT:
Jason Ramapuram, Yan Wu, Alexandros Kalousis#Kanerva++: extending the kanerva machine with differentiable, locally block allocated latent memory#2022#

VERIFIED URL:
  https://arxiv.org/abs/2103.03905

CORRECTED REFERENCE:
Jason Ramapuram, Yan Wu, Alexandros Kalousis. (2021). "Kanerva++: extending The Kanerva Machine with differentiable, locally block allocated latent memory". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/c04dca87cc9330f57a1de3a3cc7c0d2085910772.

================================================================================
REFERENCE: Variational memory addressing in generative models
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1709.07116

RAW REFERENCE TEXT:
Jörg Bornschein, Andriy Mnih, Daniel Zoran, Danilo J. Rezende#Variational memory addressing in generative models#2017#

VERIFIED URL:
  https://arxiv.org/abs/1709.07116

CORRECTED REFERENCE:
J. Bornschein, A. Mnih, Daniel Zoran, Danilo Jimenez Rezende. (2017). "Variational Memory Addressing in Generative Models". In Neural Information Processing Systems. https://www.semanticscholar.org/paper/d20af35f65fda506b605182d9b93025b559b1433.

================================================================================
REFERENCE: Addressing some limitations of transformers with feedback memory
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar#Addressing some limitations of transformers with feedback memory#2021#

================================================================================
REFERENCE: Unbounded cache model for online language modeling with open vocabulary
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/1711.02604

RAW REFERENCE TEXT:
Edouard Grave, Moustapha Cisse, Armand Joulin#Unbounded cache model for online language modeling with open vocabulary#2017#

VERIFIED URL:
  https://arxiv.org/abs/1711.02604

CORRECTED REFERENCE:
Edouard Grave, Moustapha Cissé, Armand Joulin. (2017). "Unbounded cache model for online language modeling with open vocabulary". In Neural Information Processing Systems. https://www.semanticscholar.org/paper/2ed4ebe1878fd8e421f24e1aac76fbdc89e9d381.

================================================================================
REFERENCE: Generalization through memorization: Nearest neighbor language models
Type: ❌ multiple
Details: - Year mismatch: cited as 1911 but actually 2019
- Venue mismatch: cited as 'arXiv preprint, 2019' but actually 'International Conference on Learning Representations'

RAW REFERENCE TEXT:
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis#Generalization through memorization: Nearest neighbor language models#arXiv preprint#2019#arXiv:1911.00172

VERIFIED URL:
  https://arxiv.org/abs/1911.00172

CORRECTED REFERENCE:
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, M. Lewis. (2019). "Generalization through Memorization: Nearest Neighbor Language Models". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844.

================================================================================
REFERENCE: Knowledge editing for large language models: A survey
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2310.16218

RAW REFERENCE TEXT:
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li#Knowledge editing for large language models: A survey#2023a#

VERIFIED URL:
  https://arxiv.org/abs/2310.16218

CORRECTED REFERENCE:
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li. (2023). "Knowledge Editing for Large Language Models: A Survey". In ACM Computing Surveys. https://www.semanticscholar.org/paper/42016f91e5b1da63174d45acb96bc89b64aa124d.

================================================================================
REFERENCE: Fast model editing at scale
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2021' but actually 'International Conference on Learning Representations'

RAW REFERENCE TEXT:
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning#Fast model editing at scale#arXiv preprint#2021#arXiv:2110.11309

VERIFIED URL:
  https://arxiv.org/abs/2110.11309

CORRECTED REFERENCE:
E. Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning. (2021). "Fast Model Editing at Scale". In International Conference on Learning Representations. https://www.semanticscholar.org/paper/9286ac6e9b1aacd7d93496eb4615ae7678876d2a.

================================================================================
REFERENCE: Editing factual knowledge in language models
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2021' but actually 'Conference on Empirical Methods in Natural Language Processing'

RAW REFERENCE TEXT:
Nicola De Cao, Wilker Aziz, Ivan Titov#Editing factual knowledge in language models#arXiv preprint#2021#arXiv:2104.08164

VERIFIED URL:
  https://arxiv.org/abs/2104.08164

CORRECTED REFERENCE:
Nicola De Cao, Wilker Aziz, Ivan Titov. (2021). "Editing Factual Knowledge in Language Models". In Conference on Empirical Methods in Natural Language Processing. https://www.semanticscholar.org/paper/240b0caabb415578bdea4da7d0a32bdff2e8163f.

================================================================================
REFERENCE: Mass-editing memory in a transformer
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2022b' but actually 'arXiv'

RAW REFERENCE TEXT:
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau#Mass-editing memory in a transformer#arXiv preprint#2022b#arXiv:2210.07229

VERIFIED URL:
  https://arxiv.org/abs/2210.07229

CORRECTED REFERENCE:
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau. (2022). "Mass-Editing Memory in a Transformer". In arXiv. https://arxiv.org/abs/2210.07229.

================================================================================
REFERENCE: Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models
Type: ⚠️ venue
Details: Venue mismatch: cited as 'Thirty-seventh Conference on Neural Information Processing Systems, 2023' but actually 'Neural Information Processing Systems'

RAW REFERENCE TEXT:
Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun#Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models#Thirty-seventh Conference on Neural Information Processing Systems#2023#https://openreview.net/forum?id=EldbUlZtbd

VERIFIED URL:
  https://arxiv.org/abs/2301.04213

CORRECTED REFERENCE:
Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun. (2023). "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models". In Neural Information Processing Systems. https://www.semanticscholar.org/paper/9c0a434b240299cec0029a1be93ab263d7ec9963.

================================================================================
REFERENCE: Model editing at scale leads to gradual and catastrophic forgetting
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2024' but actually 'Annual Meeting of the Association for Computational Linguistics'

RAW REFERENCE TEXT:
Akshat Gupta, Anurag Rao, Gopala Anumanchipalli#Model editing at scale leads to gradual and catastrophic forgetting#arXiv preprint#2024#arXiv:2401.07453

VERIFIED URL:
  https://arxiv.org/abs/2401.07453

CORRECTED REFERENCE:
Akshat Gupta, Anurag Rao, G. Anumanchipalli. (2024). "Model Editing at Scale leads to Gradual and Catastrophic Forgetting". In Annual Meeting of the Association for Computational Linguistics. https://www.semanticscholar.org/paper/3a716fc45fe6233a29f58ad492e21d690d9054a0.

================================================================================
REFERENCE: Unveiling the pitfalls of knowledge editing for large language models
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2310.02129

RAW REFERENCE TEXT:
Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen#Unveiling the pitfalls of knowledge editing for large language models#2023#

VERIFIED URL:
  https://arxiv.org/abs/2310.02129

CORRECTED REFERENCE:
Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Meng Wang, Xi Chen, Huajun Chen. (2023). "Unveiling the Pitfalls of Knowledge Editing for Large Language Models". In arXiv.org. https://www.semanticscholar.org/paper/427af7080ecfd7925f03439488ee0ae6aebe755b.

================================================================================
REFERENCE: Model editing can hurt general abilities of large language models
Type: ⚠️ venue
Details: Venue missing: should include 'arXiv.org'

RAW REFERENCE TEXT:
Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng#Model editing can hurt general abilities of large language models#2024#

VERIFIED URL:
  https://www.semanticscholar.org/paper/7808f614d993cae3ae2cfd3afc32f8d9191f7126

CORRECTED REFERENCE:
Jia-Chen Gu, Haoyang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng. (2024). "Model Editing Can Hurt General Abilities of Large Language Models". In arXiv.org. https://www.semanticscholar.org/paper/7808f614d993cae3ae2cfd3afc32f8d9191f7126.

================================================================================
REFERENCE: Knowledge sanitization of large language models
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2309.11852

RAW REFERENCE TEXT:
Yoichi Ishibashi, Hidetoshi Shimodaira#Knowledge sanitization of large language models#2023#

VERIFIED URL:
  https://arxiv.org/abs/2309.11852

CORRECTED REFERENCE:
Yoichi Ishibashi, H. Shimodaira. (2023). "Knowledge Sanitization of Large Language Models". In arXiv.org. https://www.semanticscholar.org/paper/db95150f29a37d8736037af7506ca7c63331e097.

================================================================================
REFERENCE: Who’s harry potter? approximate unlearning in llms
Type: ❓ unverified
Details: Reference could not be verified

RAW REFERENCE TEXT:
Ronen Eldan, Mark Russinovich#Who’s harry potter? approximate unlearning in llms#2023#

================================================================================
REFERENCE: In-context unlearning: Language models as few shot unlearners
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2023' but actually 'International Conference on Machine Learning'

RAW REFERENCE TEXT:
Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju#In-context unlearning: Language models as few shot unlearners#arXiv preprint#2023#arXiv:2310.07579

VERIFIED URL:
  https://arxiv.org/abs/2310.07579

CORRECTED REFERENCE:
Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju. (2023). "In-Context Unlearning: Language Models as Few Shot Unlearners". In International Conference on Machine Learning. https://www.semanticscholar.org/paper/6cc6d59984853e5ddcbd696c443b14244d305b50.

================================================================================
REFERENCE: Knowledge neurons in pretrained transformers
Type: ⚠️ venue
Details: Venue mismatch: cited as 'arXiv preprint, 2021' but actually 'Annual Meeting of the Association for Computational Linguistics'

RAW REFERENCE TEXT:
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei#Knowledge neurons in pretrained transformers#arXiv preprint#2021#arXiv:2104.08696

VERIFIED URL:
  https://arxiv.org/abs/2104.08696

CORRECTED REFERENCE:
Damai Dai, Li Dong, Y. Hao, Zhifang Sui, Furu Wei. (2021). "Knowledge Neurons in Pretrained Transformers". In Annual Meeting of the Association for Computational Linguistics. https://www.semanticscholar.org/paper/2c871df72c52b58f05447fcb3afc838168d94505.

================================================================================
REFERENCE: Easyedit: An easy-to-use knowledge editing framework for large language models
Type: ⚠️ venue
Details: Reference should include arXiv URL: https://arxiv.org/abs/2308.07269

RAW REFERENCE TEXT:
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen#Easyedit: An easy-to-use knowledge editing framework for large language models#2023b#

VERIFIED URL:
  https://arxiv.org/abs/2308.07269

CORRECTED REFERENCE:
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bo Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen. (2023). "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models". In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). https://www.semanticscholar.org/paper/36a364316c7bab6ed2a599ad2ce364fb2cd3e9dc.

================================================================================
